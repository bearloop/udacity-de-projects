{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project explores a rich US immigration dataset in conjuction with US demographics and airports data. The user should be able to query the data to answer questions about US visitor flows in 2016, such as **\"What large airport had the most air travelers as % of each airport's city population?\"** (see notebook end for answer).\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports and installs here\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config.get('AWS','AWS_ACCESS_KEY_ID')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config.get('AWS','AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This is to handle \"Exception: Java gateway process exited before sending its port number\" that arises in the workspace, as per https://knowledge.udacity.com/questions/573236\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build and return a Spark processs\n",
    "spark = SparkSession.builder.config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\").getOrCreate()\n",
    "#spark.sparkContext.addPyFile(\"./home/workspace/hadoop-aws-3.3.1.jar\")\n",
    "\n",
    "# Specify output S3 bucket\n",
    "s3_path='s3a://user-capstone-bucket/parquet/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "##### Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc\n",
    "\n",
    "This project utilizes immigration, demographics and airport-codes data to shed light on such questions as:\n",
    "\n",
    "-1. \"How many visitors entered each US state in 2016 as % of the existing population -- based on 2015 demographics -- ?\".\n",
    "\n",
    "-2. \"What was their mean age?\"\n",
    "\n",
    "The project connects to Spark, reads the immigration, demographics and airports data, transforms them into tables and writes them into an S3 bucket.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "##### Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "**1. Immigration dataset:** The US National Tourism and Trade office provides this dataset and includes information on visitors, their points of entry, their date of arrival etc. <a href=\"https://travel.trade.gov/research/reports/i94/historical/2016.html\" target=\"_blank\">link to the dataset</a>\n",
    "\n",
    "**2. Demographics dataset:** U.S. City Demographic Data provided by OpenSoft. This dataset includes population data by age, ethnographic group, state/city, household size etc. <a href=\"https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\" target=\"_blank\">link to the dataset</a>\n",
    "\n",
    "**3. Airports dataset:** A table of airport detailed information such as airport type, name, state code etc. Provided by datahub.io <a href=\"https://datahub.io/core/airport-codes#data\" target=\"_blank\">link to the dataset</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immigration (rows, columns): (3096313,28)\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Load immigration df\n",
    "df_immigration = spark.read.load('./sas_data')\n",
    "print_size(df_immigration, 'Immigration')\n",
    "\n",
    "df_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics (rows, columns): (2891,12)\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Demographics df\n",
    "df_demographics = spark.read.csv('us-cities-demographics.csv',sep=';',header=True)\n",
    "print_size(df_demographics, 'Demographics')\n",
    "\n",
    "df_demographics.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airports (rows, columns): (55075,12)\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Airports df\n",
    "df_airports = spark.read.csv('airport-codes_csv.csv',sep=',',header=True)\n",
    "print_size(df_airports, 'Airports')\n",
    "\n",
    "df_airports.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Template optionals\n",
    "#df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "\n",
    "#write to parquet\n",
    "#df_spark.write.parquet(\"sas_data\")\n",
    "#df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "##### Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "Both datasets have several columns that are not relevant for analysis, rows with missing values or fields that require conversion or someother  manipulation. We'll be handling those issues in the next few steps.\n",
    "\n",
    "#### Cleaning Steps\n",
    "##### Document steps necessary to clean the data\n",
    "\n",
    "Having explored the datasets, the following clean up task are performed using the respective clean_up methods retrieved from the helper script:\n",
    "\n",
    "**1. Immigration dataset:** We drop 15 columns from the original dataframe keeping only those that are relevant, convert SAS dates to date strings and drop all rows which have not reported (code=9) or have a missing mode of transport.\n",
    "\n",
    "\n",
    "**2. Demographics dataset:** Convert numeric columns'  data type from string to int or double to accommodate calculations.\n",
    "\n",
    "**3. Airports dataset:** Filter out any non-US airpor and drop the prefix \"US-\" from \"iso_region\" to use the column as key later on. Also drop all airports that have shut down. In addition, coordinates are split to latitude and longitude, and data types for elevation feet as well as lat/lon are converted to double."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immigration df post clean up (rows, columns): (2937395,13)\n",
      "+---------+------+-----+----------+-----------+-----------+------------+-----------+--------------+--------+----------------+---------+-------+\n",
      "|       id|  year|month|country_it|country_res|entry_point|date_arrival|date_depart|mode_transport|us_state|visitor_birth_yr|visa_type|col_one|\n",
      "+---------+------+-----+----------+-----------+-----------+------------+-----------+--------------+--------+----------------+---------+-------+\n",
      "|5748517.0|2016.0|  4.0|     245.0|      438.0|        LOS|  2016-04-30| 2016-05-08|           1.0|      CA|            40.0|      1.0|    1.0|\n",
      "|5748518.0|2016.0|  4.0|     245.0|      438.0|        LOS|  2016-04-30| 2016-05-17|           1.0|      NV|            32.0|      1.0|    1.0|\n",
      "|5748519.0|2016.0|  4.0|     245.0|      438.0|        LOS|  2016-04-30| 2016-05-08|           1.0|      WA|            29.0|      1.0|    1.0|\n",
      "|5748520.0|2016.0|  4.0|     245.0|      438.0|        LOS|  2016-04-30| 2016-05-14|           1.0|      WA|            29.0|      1.0|    1.0|\n",
      "|5748521.0|2016.0|  4.0|     245.0|      438.0|        LOS|  2016-04-30| 2016-05-14|           1.0|      WA|            28.0|      1.0|    1.0|\n",
      "|5748522.0|2016.0|  4.0|     245.0|      464.0|        HHW|  2016-04-30| 2016-05-05|           1.0|      HI|            57.0|      2.0|    1.0|\n",
      "|5748523.0|2016.0|  4.0|     245.0|      464.0|        HHW|  2016-04-30| 2016-05-12|           1.0|      HI|            66.0|      2.0|    1.0|\n",
      "|5748524.0|2016.0|  4.0|     245.0|      464.0|        HHW|  2016-04-30| 2016-05-12|           1.0|      HI|            41.0|      2.0|    1.0|\n",
      "|5748525.0|2016.0|  4.0|     245.0|      464.0|        HOU|  2016-04-30| 2016-05-07|           1.0|      FL|            27.0|      2.0|    1.0|\n",
      "|5748526.0|2016.0|  4.0|     245.0|      464.0|        LOS|  2016-04-30| 2016-05-07|           1.0|      CA|            26.0|      2.0|    1.0|\n",
      "|5748527.0|2016.0|  4.0|     245.0|      504.0|        NEW|  2016-04-30| 2016-05-02|           1.0|      MA|            44.0|      2.0|    1.0|\n",
      "|5748529.0|2016.0|  4.0|     245.0|      504.0|        WAS|  2016-04-30| 2016-05-22|           1.0|      VA|            38.0|      2.0|    1.0|\n",
      "|5748530.0|2016.0|  4.0|     245.0|      504.0|        LOS|  2016-04-30| 2016-05-03|           1.0|      CA|            56.0|      2.0|    1.0|\n",
      "|5748531.0|2016.0|  4.0|     245.0|      504.0|        LOS|  2016-04-30| 2016-05-03|           1.0|      CA|            38.0|      2.0|    1.0|\n",
      "|5748532.0|2016.0|  4.0|     245.0|      504.0|        MIA|  2016-04-30| 2016-05-07|           1.0|      FL|            53.0|      2.0|    1.0|\n",
      "|5748534.0|2016.0|  4.0|     245.0|      528.0|        SFR|  2016-04-30|       null|           1.0|      CA|            84.0|      2.0|    1.0|\n",
      "|5748876.0|2016.0|  4.0|     245.0|      582.0|        HOU|  2016-04-30| 2016-05-09|           1.0|      TX|            43.0|      1.0|    1.0|\n",
      "|5748877.0|2016.0|  4.0|     245.0|      582.0|        HOU|  2016-04-30| 2016-05-09|           1.0|      TX|            30.0|      1.0|    1.0|\n",
      "|5748881.0|2016.0|  4.0|     245.0|      582.0|        LOS|  2016-04-30| 2016-05-01|           1.0|      CA|            34.0|      2.0|    1.0|\n",
      "|5748885.0|2016.0|  4.0|     245.0|      690.0|        DAL|  2016-04-30| 2016-05-04|           1.0|      NV|            30.0|      2.0|    1.0|\n",
      "+---------+------+-----+----------+-----------+-----------+------------+-----------+--------------+--------+----------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performing cleaning tasks: immigration df\n",
    "\n",
    "df_immigration_clean = clean_up_immigration_df(df_immigration)\n",
    "print_size(df_immigration_clean, 'Immigration df post clean up')\n",
    "\n",
    "df_immigration_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics df post clean up (rows, columns): (2891,12)\n",
      "+----------------+--------------+----------+--------+----------+---------+------------+----------------+---------------+--------+--------------------+-----------+\n",
      "|    us_city_name| us_state_name|median_age|male_pop|female_pop|total_pop|veterans_pop|foreign_born_pop|mean_hhold_size|us_state|         ethno_group|total_count|\n",
      "+----------------+--------------+----------+--------+----------+---------+------------+----------------+---------------+--------+--------------------+-----------+\n",
      "|   Silver Spring|      Maryland|        33|   40601|     41862|    82463|        1562|           30908|            2.6|      MD|  Hispanic or Latino|      25924|\n",
      "|          Quincy| Massachusetts|        41|   44129|     49500|    93629|        4147|           32935|           2.39|      MA|               White|      58723|\n",
      "|          Hoover|       Alabama|        38|   38040|     46799|    84839|        4819|            8229|           2.58|      AL|               Asian|       4759|\n",
      "|Rancho Cucamonga|    California|        34|   88127|     87105|   175232|        5821|           33878|           3.18|      CA|Black or African-...|      24437|\n",
      "|          Newark|    New Jersey|        34|  138040|    143873|   281913|        5829|           86253|           2.73|      NJ|               White|      76402|\n",
      "|          Peoria|      Illinois|        33|   56229|     62432|   118661|        6634|            7517|            2.4|      IL|American Indian a...|       1343|\n",
      "|        Avondale|       Arizona|        29|   38712|     41971|    80683|        4815|            8355|           3.18|      AZ|Black or African-...|      11592|\n",
      "|     West Covina|    California|        39|   51629|     56860|   108489|        3800|           37038|           3.56|      CA|               Asian|      32716|\n",
      "|        O'Fallon|      Missouri|        36|   41762|     43270|    85032|        5783|            3269|           2.77|      MO|  Hispanic or Latino|       2583|\n",
      "|      High Point|North Carolina|        35|   51751|     58077|   109828|        5204|           16315|           2.65|      NC|               Asian|      11060|\n",
      "|          Folsom|    California|        40|   41051|     35317|    76368|        4187|           13234|           2.62|      CA|  Hispanic or Latino|       5822|\n",
      "|          Folsom|    California|        40|   41051|     35317|    76368|        4187|           13234|           2.62|      CA|American Indian a...|        998|\n",
      "|    Philadelphia|  Pennsylvania|        34|  741270|    826172|  1567442|       61995|          205339|           2.61|      PA|               Asian|     122721|\n",
      "|         Wichita|        Kansas|        34|  192354|    197601|   389955|       23978|           40270|           2.56|      KS|  Hispanic or Latino|      65162|\n",
      "|         Wichita|        Kansas|        34|  192354|    197601|   389955|       23978|           40270|           2.56|      KS|American Indian a...|       8791|\n",
      "|      Fort Myers|       Florida|        37|   36850|     37165|    74015|        4312|           15365|           2.45|      FL|               White|      50169|\n",
      "|      Pittsburgh|  Pennsylvania|        32|  149690|    154695|   304385|       17728|           28187|           2.13|      PA|               White|     208863|\n",
      "|          Laredo|         Texas|        28|  124305|    131484|   255789|        4921|           68427|           3.66|      TX|American Indian a...|       1253|\n",
      "|        Berkeley|    California|        32|   60142|     60829|   120971|        3736|           25000|           2.35|      CA|               Asian|      27089|\n",
      "|     Santa Clara|    California|        35|   63278|     62938|   126216|        4426|           52281|           2.75|      CA|               White|      55847|\n",
      "+----------------+--------------+----------+--------+----------+---------+------------+----------------+---------------+--------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performing cleaning tasks: demographics df\n",
    "\n",
    "df_demographics_clean = clean_up_demographics_df(df_demographics)\n",
    "\n",
    "print_size(df_demographics_clean, 'Demographics df post clean up')\n",
    "\n",
    "df_demographics_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airports df post clean up (rows, columns): (21431,11)\n",
      "+-----+-------------+--------------------+-----------+--------+------------+--------+---------+----------+-------------------+------------------+\n",
      "|ident| airport_type|        airport_name|iso_country|us_state|us_city_name|gps_code|iata_code|local_code|                lat|               lon|\n",
      "+-----+-------------+--------------------+-----------+--------+------------+--------+---------+----------+-------------------+------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|         US|      PA|    Bensalem|     00A|     null|       00A| -74.93360137939453|    40.07080078125|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|         US|      KS|       Leoti|    00AA|     null|      00AA|        -101.473911|         38.704022|\n",
      "| 00AK|small_airport|        Lowell Field|         US|      AK|Anchor Point|    00AK|     null|      00AK|     -151.695999146|       59.94919968|\n",
      "| 00AL|small_airport|        Epps Airpark|         US|      AL|     Harvest|    00AL|     null|      00AL| -86.77030181884766| 34.86479949951172|\n",
      "| 00AS|small_airport|      Fulton Airport|         US|      OK|        Alex|    00AS|     null|      00AS|        -97.8180194|        34.9428028|\n",
      "| 00AZ|small_airport|      Cordes Airport|         US|      AZ|      Cordes|    00AZ|     null|      00AZ|-112.16500091552734|34.305599212646484|\n",
      "| 00CA|small_airport|Goldstone /Gts/ A...|         US|      CA|     Barstow|    00CA|     null|      00CA|     -116.888000488|35.350498199499995|\n",
      "| 00CL|small_airport| Williams Ag Airport|         US|      CA|       Biggs|    00CL|     null|      00CL|        -121.763427|         39.427188|\n",
      "| 00CN|     heliport|Kitchen Creek Hel...|         US|      CA| Pine Valley|    00CN|     null|      00CN|       -116.4597417|        32.7273736|\n",
      "| 00FA|small_airport| Grass Patch Airport|         US|      FL|    Bushnell|    00FA|     null|      00FA| -82.21900177001953| 28.64550018310547|\n",
      "| 00FD|     heliport|  Ringhaver Heliport|         US|      FL|   Riverview|    00FD|     null|      00FD| -82.34539794921875|28.846599578857422|\n",
      "| 00FL|small_airport|   River Oak Airport|         US|      FL|  Okeechobee|    00FL|     null|      00FL| -80.96920013427734|27.230899810791016|\n",
      "| 00GA|small_airport|    Lt World Airport|         US|      GA|    Lithonia|    00GA|     null|      00GA| -84.06829833984375| 33.76750183105469|\n",
      "| 00GE|     heliport|    Caffrey Heliport|         US|      GA|       Hiram|    00GE|     null|      00GE| -84.73390197753906| 33.88420104980469|\n",
      "| 00HI|     heliport|  Kaupulehu Heliport|         US|      HI| Kailua/Kona|    00HI|     null|      00HI|        -155.980233|         19.832715|\n",
      "| 00ID|small_airport|Delta Shores Airport|         US|      ID|  Clark Fork|    00ID|     null|      00ID|-116.21399688720703|48.145301818847656|\n",
      "| 00IG|small_airport|       Goltl Airport|         US|      KS|    McDonald|    00IG|     null|      00IG|        -101.395994|         39.724028|\n",
      "| 00II|     heliport|Bailey Generation...|         US|      IN|  Chesterton|    00II|     null|      00II|   -87.122802734375|41.644500732421875|\n",
      "| 00IL|small_airport|      Hammer Airport|         US|      IL|        Polo|    00IL|     null|      00IL|  -89.5604019165039| 41.97840118408203|\n",
      "| 00IN|     heliport|St Mary Medical C...|         US|      IN|      Hobart|    00IN|     null|      00IN|  -87.2605972290039| 41.51139831542969|\n",
      "+-----+-------------+--------------------+-----------+--------+------------+--------+---------+----------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performing cleaning tasks: airports df\n",
    "\n",
    "df_airports_clean = clean_up_airports_df(df_airports)\n",
    "\n",
    "print_size(df_airports_clean, 'Airports df post clean up')\n",
    "\n",
    "df_airports_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "##### Map out the conceptual data model and explain why you chose that model\n",
    "We select to model the data using the Star Schema, which will allow end users to more effectively query the data joining the fact table and the three dimension tables. There's a detailed data dictionaryt towards the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_immigration_clean.printSchema()\n",
    "# df_demographics_clean.printSchema()\n",
    "# df_airports_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "##### List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "Here are the steps that will create our data pipeline:\n",
    "\n",
    "-1. Drop nulls and duplicates\n",
    "\n",
    "-2. Use the 3 staging tables -- **df_demographics_clean, df_immigration_clean and df_airports_clean** -- to create 3 dimension tables called **dim_city_pop, dim_airports_cities, dim_immigration** and a single **fact_table**\n",
    "\n",
    "-3. Load data to S3 Save processed dimension and fact tables in parquet for downstream query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------------+--------+----------+---------+------------+----------------+---------------+----------+---------------+-----+--------------+---------------+-------+\n",
      "|us_state|us_state_name|     us_city_name|male_pop|female_pop|total_pop|veterans_pop|foreign_born_pop|mean_hhold_size|median_age|american_native|asian|american_black|hispanic_latino|  white|\n",
      "+--------+-------------+-----------------+--------+----------+---------+------------+----------------+---------------+----------+---------------+-----+--------------+---------------+-------+\n",
      "|      AK|       Alaska|        Anchorage|  152945|    145750|   298695|       27492|           33258|           2.77|        32|          36339|36825|         23107|          27261| 212696|\n",
      "|      AL|      Alabama|       Montgomery|   94582|    106004|   200586|       14955|            9337|           2.41|        35|           1277| 6518|        121360|           6648|  73545|\n",
      "|      AL|      Alabama|       Huntsville|   91764|     97350|   189114|       16637|           12691|           2.18|        38|           1755| 6566|         61561|          10887| 121904|\n",
      "|      AL|      Alabama|           Mobile|   91275|    103030|   194305|       11939|            7234|            2.4|        38|           2816| 5518|         96397|           5229|  93755|\n",
      "|      AL|      Alabama|       Birmingham|  102122|    112789|   214911|       13212|            8258|           2.21|        35|           1319| 1500|        157985|           8940|  51728|\n",
      "|      AL|      Alabama|           Hoover|   38040|     46799|    84839|        4819|            8229|           2.58|        38|           null| 4759|         18191|           3430|  61869|\n",
      "|      AL|      Alabama|           Dothan|   32172|     35364|    67536|        6334|            1699|           2.59|        38|            656| 1175|         23243|           1704|  43516|\n",
      "|      AL|      Alabama|       Tuscaloosa|   47293|     51045|    98338|        3647|            4706|           2.67|        29|            261| 2733|         42331|           2475|  52603|\n",
      "|      AR|     Arkansas|       Fort Smith|   43346|     44849|    88195|        3408|           13177|           2.44|        34|           2993| 6228|          9851|          17104|  66004|\n",
      "|      AR|     Arkansas|North Little Rock|   31671|     34835|    66506|        4130|            2787|           2.62|        33|            713| null|         30766|           4860|  34118|\n",
      "|      AR|     Arkansas|      Little Rock|   96997|    100989|   197986|       12343|           16640|           2.36|        36|            961| 8423|         85606|          15500| 102312|\n",
      "|      AR|     Arkansas|     Fayetteville|   41959|     40873|    82832|        4744|            6313|           2.28|        27|           2058| 4707|          6927|           5535|  68830|\n",
      "|      AR|     Arkansas|        Jonesboro|   35666|     38240|    73906|        3682|            3222|           2.44|        32|           2109| 1282|         14599|           4614|  56626|\n",
      "|      AR|     Arkansas|       Springdale|   36840|     43614|    80454|        3397|           19969|           3.04|        31|            547| 1422|          1859|          30200|  56843|\n",
      "|      AZ|      Arizona|          Phoenix|  786833|    776168|  1563001|       72388|          300702|           2.89|        33|          41748|66403|        132939|         669914|1161455|\n",
      "|      AZ|      Arizona|             Yuma|   48298|     45847|    94145|        7182|           19326|           2.64|        33|           1228| 1180|          3731|          57054|  69691|\n",
      "|      AZ|      Arizona|            Tempe|   91350|     84476|   175826|        7058|           26620|           2.44|        28|           4636|17338|         11293|          44404| 130346|\n",
      "|      AZ|      Arizona|         Surprise|   62875|     65567|   128442|       11109|            9929|            2.9|        39|           1883| 3153|          9079|          25366| 115419|\n",
      "|      AZ|      Arizona|         Chandler|  128885|    131948|   260833|       15870|           38721|           2.78|        35|           6601|33079|         16163|          57159| 207993|\n",
      "|      AZ|      Arizona|           Tucson|  264893|    266781|   531674|       38182|           82220|           2.45|        33|          24409|24689|         33900|         231025| 404342|\n",
      "+--------+-------------+-----------------+--------+----------+---------+------------+----------------+---------------+----------+---------------+-----+--------------+---------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+-------------+--------------------+------------+--------+----------+-------------------+------------------+\n",
      "|ident| airport_type|        airport_name|us_city_name|us_state|local_code|                lat|               lon|\n",
      "+-----+-------------+--------------------+------------+--------+----------+-------------------+------------------+\n",
      "| 02LA|     heliport|La State Police T...|Bossier City|      LA|      02LA| -93.66020202636719| 32.53129959106445|\n",
      "| 0CL5|     heliport| The Atrium Heliport|      Irvine|      CA|      0CL5|     -117.858001709|     33.6706008911|\n",
      "| 35OI|     heliport|Fawcett Center Fo...|    Columbus|      OH|      35OI|   -83.018798828125| 40.01060104370117|\n",
      "| 36CL|     heliport|Tri-City Hospital...|   Oceanside|      CA|      36CL|-117.29100036621094|33.185298919677734|\n",
      "| 3CL1|     heliport|Metropolitan Wate...| Los Angeles|      CA|      3CL1|-118.23699951171875| 34.05440139770508|\n",
      "| 4TA1|small_airport|Warschun Ranch Ai...|      Denton|      TX|      4TA1| -97.11139678955078| 33.30289840698242|\n",
      "| 56CA|     heliport|Los Angeles Times...| Los Angeles|      CA|      56CA|-118.24600219726562| 34.05059814453125|\n",
      "| 72TX|     heliport|Johnson Space Cen...|     Houston|      TX|      72TX|  -95.0907974243164| 29.56220054626465|\n",
      "| 97IS|     heliport|St Johns Hospital...| Springfield|      IL|      97IS| -89.64440155029297| 39.80670166015625|\n",
      "| 9TS8|     heliport|Dallas Rehabilita...|      Dallas|      TX|      9TS8| -96.87889862060547| 32.84870147705078|\n",
      "| CD00|     heliport|Emancipation Hill...|     Boulder|      CO|      CD00|-105.36000061035156|  40.0536003112793|\n",
      "| FL94|     heliport|Florida Power & L...|       Miami|      FL|      FL94| -80.34499235450001|     25.7682331637|\n",
      "| K2R2|small_airport|Hendricks County ...|Indianapolis|      IN|       2R2|       -86.47380066|       39.74810028|\n",
      "| KL65|small_airport|Perris Valley Air...|      Perris|      CA|       L65|-117.21800231933594| 33.76089859008789|\n",
      "|  MO2|small_airport|Flying Bar H Ranc...| Springfield|      MO|       MO2| -93.12889862060547|37.175201416015625|\n",
      "| NH12|seaplane_base| Evans Seaplane Base|  Manchester|      NH|      NH12|         -71.468561|         43.038295|\n",
      "| 17MI|     heliport|Grace Hospital He...|     Detroit|      MI|      17MI| -83.18299865722656| 42.41780090332031|\n",
      "| 1CA7|     heliport|L. A. Times Costa...|  Costa Mesa|      CA|      1CA7|-117.91699981689453| 33.69390106201172|\n",
      "| 2TN6|     heliport|Saint Thomas Midt...|   Nashville|      TN|      2TN6|         -86.802022|         36.153953|\n",
      "| 4CA7|small_airport|     Johnsen Airport|       Chico|      CA|      4CA7|-121.88600158691406| 39.67070007324219|\n",
      "+-----+-------------+--------------------+------------+--------+----------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+--------------+---------+----+-----+--------------+\n",
      "|us_state|mode_transport|visa_type|year|month|total_visitors|\n",
      "+--------+--------------+---------+----+-----+--------------+\n",
      "|      ..|             1|        1|2016|    4|            10|\n",
      "|      ..|             1|        2|2016|    4|            20|\n",
      "|      .C|             1|        2|2016|    4|            16|\n",
      "|      .C|             1|        1|2016|    4|             1|\n",
      "|      .D|             1|        2|2016|    4|             1|\n",
      "|      .D|             1|        1|2016|    4|             1|\n",
      "|      .I|             1|        2|2016|    4|             3|\n",
      "|      .I|             1|        1|2016|    4|             1|\n",
      "|      .L|             1|        1|2016|    4|             1|\n",
      "|      .L|             1|        2|2016|    4|             1|\n",
      "|      .M|             1|        2|2016|    4|             2|\n",
      "|      .N|             1|        1|2016|    4|             1|\n",
      "|      .N|             1|        2|2016|    4|             3|\n",
      "|      .T|             1|        2|2016|    4|             6|\n",
      "|       0|             1|        1|2016|    4|            17|\n",
      "|       0|             1|        2|2016|    4|             9|\n",
      "|      00|             1|        2|2016|    4|             1|\n",
      "|      02|             1|        2|2016|    4|             1|\n",
      "|      03|             1|        1|2016|    4|             1|\n",
      "|      06|             1|        2|2016|    4|             1|\n",
      "+--------+--------------+---------+----+-----+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+--------------+---------+------------+-------------+--------------------+\n",
      "|us_state|mode_transport|visa_type|us_city_name| airport_type|        airport_name|\n",
      "+--------+--------------+---------+------------+-------------+--------------------+\n",
      "|      NC|             1|        3|   Asheville|small_airport|    Six Oaks Airport|\n",
      "|      NC|             1|        3|   Asheville|large_airport|Asheville Regiona...|\n",
      "|      NC|             1|        3|   Asheville|     heliport|Mission Hospitals...|\n",
      "|      NC|             3|        3|   Asheville|small_airport|    Six Oaks Airport|\n",
      "|      NC|             3|        3|   Asheville|large_airport|Asheville Regiona...|\n",
      "|      NC|             3|        3|   Asheville|     heliport|Mission Hospitals...|\n",
      "|      NC|             3|        1|   Asheville|small_airport|    Six Oaks Airport|\n",
      "|      NC|             3|        1|   Asheville|large_airport|Asheville Regiona...|\n",
      "|      NC|             3|        1|   Asheville|     heliport|Mission Hospitals...|\n",
      "|      NC|             1|        1|   Asheville|small_airport|    Six Oaks Airport|\n",
      "|      NC|             1|        1|   Asheville|large_airport|Asheville Regiona...|\n",
      "|      NC|             1|        1|   Asheville|     heliport|Mission Hospitals...|\n",
      "|      NC|             2|        1|   Asheville|small_airport|    Six Oaks Airport|\n",
      "|      NC|             2|        1|   Asheville|large_airport|Asheville Regiona...|\n",
      "|      NC|             2|        1|   Asheville|     heliport|Mission Hospitals...|\n",
      "|      NC|             2|        2|   Asheville|small_airport|    Six Oaks Airport|\n",
      "|      NC|             2|        2|   Asheville|large_airport|Asheville Regiona...|\n",
      "|      NC|             2|        2|   Asheville|     heliport|Mission Hospitals...|\n",
      "|      NC|             3|        2|   Asheville|small_airport|    Six Oaks Airport|\n",
      "|      NC|             3|        2|   Asheville|large_airport|Asheville Regiona...|\n",
      "+--------+--------------+---------+------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_city_pop = df_demographics_clean.groupBy('us_state','us_state_name','us_city_name','male_pop','female_pop','total_pop','veterans_pop',\n",
    "                                             'foreign_born_pop','mean_hhold_size','median_age').pivot(\"ethno_group\").mean('total_count').sort('us_state'\n",
    "                                                             ).withColumnRenamed(\"Asian\", \"asian\"\n",
    "                                                             ).withColumnRenamed(\"American Indian and Alaska Native\", \"american_native\"\n",
    "                                                             ).withColumnRenamed(\"Black or African-American\", \"american_black\"\n",
    "                                                             ).withColumnRenamed(\"Hispanic or Latino\", \"hispanic_latino\"\n",
    "                                                             ).withColumnRenamed(\"White\", \"white\").dropDuplicates()\n",
    "\n",
    "dim_city_pop = dim_city_pop.withColumn(\"asian\", dim_city_pop[\"asian\"].cast(IntegerType())\n",
    "           ).withColumn(\"american_native\", dim_city_pop[\"american_native\"].cast(IntegerType())\n",
    "           ).withColumn(\"american_black\", dim_city_pop[\"american_black\"].cast(IntegerType())\n",
    "           ).withColumn(\"hispanic_latino\", dim_city_pop[\"hispanic_latino\"].cast(IntegerType())\n",
    "           ).withColumn(\"white\", dim_city_pop[\"white\"].cast(IntegerType()))\n",
    "\n",
    "dim_airports_cities = df_airports_clean.join(dim_city_pop, ['us_state','us_city_name'], \"inner\").select(['ident', 'airport_type', 'airport_name', 'us_city_name', \n",
    "                                                                                      'us_state', 'local_code', 'lat', 'lon']).dropDuplicates()\n",
    "\n",
    "dim_immigration = df_immigration_clean.groupby('us_state','mode_transport','visa_type','year','month').sum('col_one').sort('us_state'\n",
    "                                                        ).withColumnRenamed(\"sum(col_one)\", \"total_visitors\").dropDuplicates()\n",
    "\n",
    "dim_immigration = dim_immigration.withColumn(\"mode_transport\", dim_immigration[\"mode_transport\"].cast(IntegerType())\n",
    "           ).withColumn(\"visa_type\", dim_immigration[\"visa_type\"].cast(IntegerType())\n",
    "           ).withColumn(\"year\", dim_immigration[\"year\"].cast(IntegerType())\n",
    "           ).withColumn(\"month\", dim_immigration[\"month\"].cast(IntegerType())\n",
    "           ).withColumn(\"total_visitors\", dim_immigration[\"total_visitors\"].cast(IntegerType()))\n",
    "\n",
    "sup = dim_immigration.join(dim_city_pop,['us_state']).select(['us_state','mode_transport', 'visa_type', 'us_city_name']).dropDuplicates()\n",
    "fact_table = sup.join(dim_airports_cities,['us_city_name','us_state']).select(['us_state','mode_transport', 'visa_type', 'us_city_name','airport_type', 'airport_name']).dropDuplicates()\n",
    "\n",
    "dim_city_pop.show()\n",
    "dim_airports_cities.show()\n",
    "dim_immigration.show()\n",
    "fact_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    " \n",
    "**Here we're performing two tests:\n",
    "-1. whether the key us_state has any missing values in any of the tables\n",
    "-2. whether the fact table has any missing values in any of its columns**\n",
    "\n",
    "\n",
    "**Both are important to check whether user aggregations will be succesful**\n",
    "\n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing data in unique key us_state\n"
     ]
    }
   ],
   "source": [
    "# Quality Check 1\n",
    "\n",
    "qual_1 = [fact_table.where(F.col('us_state').isNull()).count(),\n",
    "          dim_airports_cities.where(F.col('us_state').isNull()).count(),\n",
    "          dim_city_pop.where(F.col('us_state').isNull()).count(),\n",
    "          dim_immigration.where(F.col('us_state').isNull()).count()]\n",
    "\n",
    "if sum(qual_1) == 0:\n",
    "    print('No missing data in unique key us_state')\n",
    "else:\n",
    "    raise('Important Error: Unique key us_state includes null values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fact table doesn't have any missing values\n"
     ]
    }
   ],
   "source": [
    "# Quality Check 2\n",
    "if fact_table.na.drop().count() == fact_table.count():\n",
    "    print('The fact table doesn\\'t have any missing values')\n",
    "else:\n",
    "    raise('Important Error: fact_table has missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Add tables to S3 as parquet\n",
    "#####  After implementing this https://gist.github.com/eddies/f37d696567f15b33029277ee9084c4a0, using VIM,\n",
    "##### because of this error appearing in the workspace: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_city_pop.write.parquet(s3_path+'dim_city_pop.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_airports_cities.write.parquet(s3_path+'dim_airports_cities.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_immigration.write.parquet(s3_path+'dim_immigration.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_table.write.parquet(s3_path+'fact_table.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "fact_table.printSchema dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "#### dimension tables\n",
    "\n",
    "##### dim_city_pop\n",
    "\n",
    "| Field name | Type| Constraint | Description |\n",
    "| --- | --- | --- | --- |\n",
    "| us_state | string | Composite Key | US state id\n",
    "| us_city_name | string | Composite Key | US city name\n",
    "| male_pop | integer |   | Male population\n",
    "| female_pop | integer |   | Female population\n",
    "| total_pop | integer |   | Total population\n",
    "| veterans_pop | integer |   | Veterans population\n",
    "| foreign_born_pop | integer |   | Population born outside the US\n",
    "| mean_hhold_size | double |   | Average number of persons forming a household\n",
    "| median_age | integer |   | Population's mediana age\n",
    "| american_native | integer |   | Ethnographic group population: American Indian and Alaska Native\n",
    "| asian | integer |   | Ethnographic group population: Asian\n",
    "| american_black | integer |   | Ethnographic group population: Black or African-American\n",
    "| hispanic_latino | integer |   | Ethnographic group population: Hispanic or Latino\n",
    "| white | integer |   | Ethnographic group population: White\n",
    "\n",
    "\n",
    "##### dim_airports_cities\n",
    "\n",
    "| Field name | Type| Constraint | Description |\n",
    "| --- | --- | --- | --- |\n",
    "| ident | string |  | Visitor's id\n",
    "| airport_type | string |  | Airport type e.g. large airports or helicopter airports\n",
    "| airport_name | string |  Composite Key | The airport's name\n",
    "| us_state | string | Composite Key | US state id\n",
    "| us_city_name | string | Composite Key  | US city name\n",
    "| local_code | string |   | Airport id\n",
    "| lat | double |   | Airport latitude\n",
    "| lon | double |   | Airport longitude\n",
    "\n",
    "\n",
    "##### dim_immigration\n",
    "\n",
    "| Field name | Type| Constraint | Description |\n",
    "| --- | --- | --- | --- |\n",
    "| us_state | string | Composite Key | US state id\n",
    "| mode_transport | integer |  Composite Key | Id that corresponds to a mode of transport (1 = Air; 2 = Sea; 3 = Land)\n",
    "| visa_type | integer | Composite Key  | Id that corresponds to the visa type the traveler holds (1 = Business; 2 = Pleasure; 3 = Student)\n",
    "| year | integer |   | Year the trip took place\n",
    "| month | integer |   | Month the trip took place\n",
    "| total_visitors | integer |   | Number of total travelers\n",
    "\n",
    "\n",
    "##### fact_table\n",
    "\n",
    "| Field name | Type| Constraint | Description |\n",
    "| --- | --- | --- | --- |\n",
    "| us_state | string | Foreign Key | US state id\n",
    "| mode_transport | integer |  Foreign Key | Id that corresponds to a mode of transport (1 = Air; 2 = Sea; 3 = Land)\n",
    "| visa_type | integer | Foreign Key  | Id that corresponds to the visa type the traveler holds (1 = Business; 2 = Pleasure; 3 = Student)\n",
    "| us_city_name | string | Foreign Key | US city name\n",
    "| airport_type | string | Foreign Key  | Airport type e.g. large airports or helicopter airports\n",
    "| airport_name | string |  Foreign Key | The airport's name\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "**Clearly state the rationale for the choice of tools and technologies for the project.**\n",
    "\n",
    "We decided to use Spark for data wrangling and cleaning because of its power when it comes to handling large datasets. We used S3 to store the tables due to lower costs as  we're very budget constraint and couldn't afford a Redshift cluster.\n",
    "\n",
    "**Propose how often the data should be updated and why.**\n",
    "\n",
    "The data can be updated even daily assuming the immigration dataset gets updated in that frequency. However, since we're creating monthly aggregation it might make more sense to run the pipeline once per month.\n",
    "\n",
    "Write a description of how you would approach the problem differently under the following scenarios:\n",
    "\n",
    "**The data was increased by 100x.**\n",
    "\n",
    "We could use larger EC2 instances in AWS's EMR or additional work nodes. Increasing capacity is usually the fastest way to improve perfomance and deal with any issues a 100x dataset might bring about.\n",
    "\n",
    "**The data populates a dashboard that must be updated on a daily basis by 7am every day.**\n",
    "\n",
    "In that case, using Airflow for scheduling and automating the data pipeline jobs is probably a good solution\n",
    "sers.\n",
    " \n",
    "**The database needed to be accessed by 100+ people.**\n",
    " \n",
    "The larger the number of people accessing a database, the more likely to face perfomrance issues. Again adding more nodes or usinglarger Redshift clusters is probably the easiest way to deal with the higher workload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 6: Sample query\n",
    "\n",
    "#### What large airport had the most air travelers as % of each airport's city population?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+--------------------+-------------------+---------+------------------+\n",
      "|us_state|   us_city_name|        airport_name|sum(total_visitors)|total_pop|             ratio|\n",
      "+--------+---------------+--------------------+-------------------+---------+------------------+\n",
      "|      FL|     Fort Myers|Southwest Florida...|             616842|    74015| 8.334013375667094|\n",
      "|      FL|West Palm Beach|Palm Beach Intern...|             616842|   106782|5.7766477496207225|\n",
      "|      CA|      Fairfield|Travis Air Force ...|             462964|   112972|4.0980419927061575|\n",
      "|      NY|       Syracuse|Syracuse Hancock ...|             542397|   144152| 3.762674121760364|\n",
      "|      FL|Fort Lauderdale|Fort Lauderdale H...|             616842|   178587|  3.45401400997833|\n",
      "|      FL|    Tallahassee|Tallahassee Regio...|             616842|   189894|3.2483490789598406|\n",
      "|      CA|        Ontario|Ontario Internati...|             462964|   171200|2.7042289719626167|\n",
      "|      NY|      Rochester|Greater Rochester...|             542397|   209808|2.5852064744909633|\n",
      "|      FL|        Orlando|Orlando Sanford I...|             616842|   270917|2.2768670847528947|\n",
      "|      FL|        Orlando|Orlando Internati...|             616842|   270917|2.2768670847528947|\n",
      "|      NY|        Buffalo|Buffalo Niagara I...|             542397|   258066|2.1017762897863337|\n",
      "|      FL|          Tampa|Tampa Internation...|             616842|   369028|1.6715316994916376|\n",
      "|      FL|          Tampa|Mac Dill Air Forc...|             616842|   369028|1.6715316994916376|\n",
      "|      FL|          Miami|Miami Internation...|             616842|   440989|1.3987695838218186|\n",
      "|      CA|      Santa Ana|John Wayne Airpor...|             462964|   335423|1.3802392799539687|\n",
      "|      TX|  Wichita Falls|Sheppard Air Forc...|             130733|   104709|1.2485364199830005|\n",
      "|      CA|        Oakland|Metropolitan Oakl...|             462964|   419278| 1.104193399128979|\n",
      "|      TX|        Abilene|Dyess Air Force Base|             130733|   125876|1.0385855921700722|\n",
      "|      CA|     Sacramento|Sacramento Intern...|             462964|   490715|0.9434478261312574|\n",
      "|      FL|   Jacksonville|Jacksonville Inte...|             616842|   868031|0.7106220860775709|\n",
      "+--------+---------------+--------------------+-------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_table.join(dim_immigration,['us_state','mode_transport','visa_type'],'inner').filter((F.col('mode_transport')==1) & (F.col('year') == 2016) & (F.col('airport_type')=='large_airport')\n",
    "                ).groupBy('us_state','us_city_name','airport_name').sum('total_visitors'\n",
    "                ).join(dim_city_pop,['us_state','us_city_name']).select('us_state','us_city_name','airport_name','sum(total_visitors)','total_pop').sort('airport_name'\n",
    "                ).withColumn(\"ratio\", F.col(\"sum(total_visitors)\") / F.col(\"total_pop\")).sort(F.desc('ratio')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
